# BeyondChats Article Automation System

A complete article scraping, optimization, and display system with three distinct components working together through a REST API.

---

## ğŸ“‹ Table of Contents

- [Overview](#overview)
- [Project Structure](#project-structure)
- [Task 1: Scrape & Store Articles](#task-1-scrape--store-articles)
- [Task 2: Generate Optimized Articles](#task-2-generate-optimized-articles)
- [Task 3: Frontend Display](#task-3-frontend-display)
- [How to Run](#how-to-run)
- [System Architecture](#system-architecture)
- [API Reference](#api-reference)

---

## Overview

This system implements three independent but connected tasks:

1. **Scrape oldest articles** from BeyondChats blog â†’ Store in database via API
2. **Fetch latest article** â†’ Search Google â†’ Scrape competitors â†’ Generate improved version with LLM â†’ Publish with citations
3. **Display articles** in a React frontend with clear visual distinction between original and generated content

**Technology Stack:** Node.js, Express, SQLite, React, TypeScript, Groq LLM API

---

## Project Structure

```
beyondchats/
â”œâ”€â”€ backend/                    # Express API + SQLite database
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ server.js          # Main server entry point
â”‚   â”‚   â”œâ”€â”€ controllers/       # API request handlers
â”‚   â”‚   â”œâ”€â”€ models/            # Database models
â”‚   â”‚   â”œâ”€â”€ routes/            # API routes
â”‚   â”‚   â””â”€â”€ db/                # Database setup
â”‚   â”œâ”€â”€ articles.db            # SQLite database (created on first run)
â”‚   â””â”€â”€ package.json
â”‚
â”œâ”€â”€ scripts/                   # Automation scripts (Tasks 1 & 2)
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ index.js          # Main entry point (runs both tasks)
â”‚   â”‚   â”‚
â”‚   â”‚   # TASK 1 FILES:
â”‚   â”‚   â”œâ”€â”€ scrape.js         # Scrapes oldest articles from BeyondChats
â”‚   â”‚   â”œâ”€â”€ rewrite.js        # Saves scraped articles to backend
â”‚   â”‚   â”‚
â”‚   â”‚   # TASK 2 FILES:
â”‚   â”‚   â”œâ”€â”€ fetchLatest.js    # Fetches latest article from backend
â”‚   â”‚   â”œâ”€â”€ googleSearch.js   # Searches Google & scrapes competitors
â”‚   â”‚   â”œâ”€â”€ llmRewriter.js    # Rewrites article using Groq LLM
â”‚   â”‚   â””â”€â”€ publishArticle.js # Publishes generated article to backend
â”‚   â”‚
â”‚   â”œâ”€â”€ .env                   # Configuration (API keys)
â”‚   â””â”€â”€ package.json
â”‚
â”œâ”€â”€ frontend/                  # React + TypeScript UI
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ App.tsx           # Main component with all logic
â”‚   â”‚   â”œâ”€â”€ App.css           # Styling
â”‚   â”‚   â””â”€â”€ main.tsx          # Entry point
â”‚   â””â”€â”€ package.json
â”‚
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ architecture.jpeg      # Visual architecture diagram
â”‚
â””â”€â”€ README.md                  # This file
```

**Key Points:**
- Only **3 top-level folders**: `backend/`, `scripts/`, `frontend/`
- `scripts/` contains BOTH Task 1 and Task 2 in a single automated flow
- Clear file names indicate purpose
- No duplicate or test files

---

## Task 1: Scrape & Store Articles

### What It Does

1. Loads https://beyondchats.com/blogs/
2. Detects pagination and navigates to the **last page** (oldest articles)
3. Scrapes the **5 oldest articles**
4. Saves them to the database via backend CRUD API

### Files Involved

| File | Purpose |
|------|---------|
| `scripts/src/scrape.js` | Scraping logic with cheerio |
| `scripts/src/rewrite.js` | Sends articles to backend API |
| `scripts/src/index.js` | Orchestrates the flow |

### Flow

```
BeyondChats Blog
    â†“
scrape.js (extract articles)
    â†“
rewrite.js (POST to /api/articles)
    â†“
Backend API
    â†“
SQLite Database
```

### Database Schema

```sql
CREATE TABLE articles (
  id TEXT PRIMARY KEY,
  title TEXT NOT NULL,
  content TEXT NOT NULL,
  source_url TEXT,
  source_type TEXT DEFAULT 'beyondchats',
  is_generated INTEGER DEFAULT 0,
  original_article_id TEXT,
  references TEXT,
  created_at TEXT NOT NULL,
  updated_at TEXT NOT NULL
)
```

---

## Task 2: Generate Optimized Articles

### What It Does

1. **Fetches** the latest article from backend API
2. **Searches** Google for the article title
3. **Scrapes** the first 2 blog/article results from Google
4. **Calls Groq LLM** to rewrite the original article by:
   - Analyzing competitor content
   - Improving formatting and comprehensiveness
   - Matching style of top-ranking content
5. **Publishes** the generated article back to backend
6. **Cites** the 2 competitor articles at the bottom

### Files Involved

| File | Purpose |
|------|---------|
| `scripts/src/fetchLatest.js` | GET latest article from backend |
| `scripts/src/googleSearch.js` | Google search + scrape competitors |
| `scripts/src/llmRewriter.js` | LLM API call (Groq) |
| `scripts/src/publishArticle.js` | POST generated article to backend |
| `scripts/src/index.js` | Orchestrates the flow |

### Flow

```
Backend API (GET /api/articles)
    â†“
fetchLatest.js (get latest article)
    â†“
googleSearch.js (search + scrape 2 competitors)
    â†“
llmRewriter.js (call Groq LLM API)
    â†“
publishArticle.js (POST /api/articles)
    â†“
Backend API
    â†“
Database (with references field populated)
```

### LLM Integration

- **API Used:** Groq (free tier with generous limits)
- **Model:** Llama 3.1 70B Versatile
- **Fallback:** If no API key, uses demo mode
- **Get API Key:** https://console.groq.com/keys

### How Generated Articles Are Tracked

- `is_generated = 1` â†’ Marks AI-generated content
- `original_article_id` â†’ Links to source article
- `references` â†’ JSON array of competitor URLs
- `source_type = 'llm_generated'`

---

## Task 3: Frontend Display

### What It Does

Displays all articles (original and generated) in a clean, responsive React interface.

### Features

- **List View:** All articles as clickable cards
- **Visual Distinction:**
  - Original articles â†’ Blue border
  - Generated articles â†’ Purple border
- **Detail View:** Full content with metadata
- **References:** Shows cited sources for generated articles
- **Relationship Links:** Navigate between original â†” generated
- **Responsive Design:** Works on all screen sizes

### Files Involved

| File | Purpose |
|------|---------|
| `frontend/src/App.tsx` | Main component (list + detail views) |
| `frontend/src/App.css` | All styling |
| `frontend/src/main.tsx` | Renders React app |

### API Endpoints Used

- `GET http://localhost:3000/api/articles` â†’ Fetch all articles

---

## How to Run

### Prerequisites

- Node.js 14+ installed
- npm installed

### Step 1: Start Backend Server

```bash
cd backend
npm install
npm start
```

**Expected output:**
```
Server running on http://localhost:3000
```

**Keep this terminal running!**

---

### Step 2: Run Scripts (Tasks 1 & 2)

Open a **new terminal** and run:

```bash
cd scripts
npm install
npm start
```

**What happens:**
1. Scrapes 5 oldest articles from BeyondChats (Task 1)
2. Saves them to database
3. Fetches latest article (Task 2)
4. Searches Google for competitors
5. Generates optimized version with LLM
6. Saves generated article with citations

**Expected output:**
```
=== Starting BeyondChats Article Automation ===

ğŸ“‹ MODE: Scraping new articles from BeyondChats...
âœ” Found 5 articles
âœ” New articles saved to backend

============================================================
ğŸ“ MODE: Processing latest article with SEO optimization...

âœ” Latest article: "Can Chatbots Boost Small Business Growth?"
ğŸ“ Starting Google search...
âœ” Scraped 2 articles from Google search results
ğŸ¤– Calling LLM to rewrite article...
âœ” Article successfully rewritten by LLM (Groq)
ğŸ“¤ Publishing enhanced article to backend...
âœ” Article published successfully!

ğŸ‰ SUCCESS! Article optimization complete.
```

---

### Step 3: Start Frontend

Open a **third terminal** and run:

```bash
cd frontend
npm install
npm run dev
```

Open your browser to: **http://localhost:5173**

---

## System Architecture

### High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  BeyondChats    â”‚
â”‚   Blog (Web)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â†“ (Task 1: Scrape)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Scripts        â”‚â†â”€â”€â”€â”€â”€â†’â”‚  Backend     â”‚
â”‚  (Automation)   â”‚  API  â”‚  (Express)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚
         â†“ (Task 2)              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Google Search  â”‚       â”‚  SQLite DB   â”‚
â”‚  + Groq LLM     â”‚       â”‚  (articles)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â†“ (Task 3)
                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                          â”‚   Frontend   â”‚
                          â”‚   (React)    â”‚
                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Flow

**Task 1 Flow:**
```
BeyondChats â†’ scrape.js â†’ rewrite.js â†’ Backend API â†’ Database
```

**Task 2 Flow:**
```
Database â†’ fetchLatest.js â†’ googleSearch.js â†’ llmRewriter.js â†’ publishArticle.js â†’ Database
```

**Task 3 Flow:**
```
Database â†’ Backend API â†’ Frontend (React) â†’ Browser
```

See `docs/architecture.jpeg` for visual diagram.

---

## API Reference

### Backend Endpoints

#### GET `/api/articles`
Returns all articles (newest first)

**Response:**
```json
[
  {
    "id": "550e8400-e29b-41d4-a716-446655440000",
    "title": "Article Title",
    "content": "Full article content...",
    "source_url": "https://beyondchats.com/blogs/article",
    "source_type": "beyondchats",
    "is_generated": 0,
    "original_article_id": null,
    "references": null,
    "created_at": "2025-12-27T00:00:00.000Z",
    "updated_at": "2025-12-27T00:00:00.000Z"
  }
]
```

#### GET `/api/articles/:id`
Returns a single article by ID

#### POST `/api/articles`
Creates a new article

**Request body:**
```json
{
  "title": "Article Title",
  "content": "Article content...",
  "source_url": "https://example.com",
  "source_type": "beyondchats",
  "is_generated": 0
}
```

#### PATCH `/api/articles/:id`
Updates an existing article

#### DELETE `/api/articles/:id`
Deletes an article

---

## Configuration

### Environment Variables (`scripts/.env`)

```bash
# LLM API (Groq - Free)
GROQ_API_KEY=gsk_xxxxx

# Backend URL
BACKEND_URL=http://localhost:3000

# Optional: Google Search APIs
SERPAPI_KEY=xxxxx                    # serpapi.com (100 free/month)
GOOGLE_SEARCH_API_KEY=xxxxx          # Google Custom Search
GOOGLE_SEARCH_ENGINE_ID=xxxxx
```

**Get Free API Keys:**
- Groq: https://console.groq.com/keys (required for Task 2)
- SerpAPI: https://serpapi.com (optional, improves Google search)

---

## Troubleshooting

### Backend not starting
- **Issue:** Port 3000 already in use
- **Solution:** Kill process: `lsof -ti:3000 | xargs kill -9`

### No articles in database
- **Issue:** Database is empty
- **Solution:** Run `cd scripts && npm start` to scrape articles

### "Using demo rewrite mode"
- **Issue:** GROQ_API_KEY not configured
- **Solution:** Add API key to `scripts/.env`

### Frontend errors
- **Issue:** Cannot connect to backend
- **Solution:** Ensure backend is running on port 3000

### "Using demo competitor articles"
- **Issue:** Google search API not configured (normal)
- **Solution:** System uses fallback demo mode or DuckDuckGo. For production, add SERPAPI_KEY

---

## Next Steps (Future Enhancements)

- Add automated tests
- Implement real-time Google search with proper API
- Add authentication and user management
- Deploy to production (Vercel + Railway)
- Add article scheduling and automated generation
- Implement caching for better performance
